\chapter{SM Second}
\section{Maxwell-Boltzmann Statistics}
Consider a system consisting of a large number of particles which are identical and distinguishable. Consider an ideal gas taken in a vessel of volume V. The particles (molecules) in the gas are in random motion, they move in all possible directions with all possible velocities, colliding with each other and with the walls of the vessel. The collisions are considered to be elastic. But during each collision the energy and momentum of each particle change. The change in momentum and energy is continuous, but total energy of the system remains constant.\\
\par Let $\mathrm{n}_{1}, \mathrm{n}_{2}, \mathrm{n}_{3}, \ldots . \mathrm{n}_{\mathrm{i}} \ldots$ be the number of particles in the energy levels $\epsilon_{1}, \in_{2}, \in_{3} \cdots \epsilon_{i} \cdots$. The total number of particles and total energy of the system are constants.
\begin{equation}
\text{So, }\mathrm{n}_{1}+\mathrm{n}_{2}+\mathrm{n}_{3}+\ldots .+\mathrm{n}_{\mathrm{i}}+\ldots=\sum \mathrm{n}_{\mathrm{i}}=\mathrm{n}=\mathrm{a}\text{ constant}
\end{equation}
\begin{equation}
\mathrm{n}_{1} \epsilon_{1}+\mathrm{n}_{2} \epsilon_{2}+\ldots .+\mathrm{n}_{\mathrm{i}} \epsilon_{\mathrm{i}}+\ldots .=\sum \mathrm{n}_{\mathrm{i}} \epsilon_{\mathrm{i}}=\mathrm{E}=\text { a constant }
\end{equation}
\par Let us consider a group of adjacent cells in the phase space. Such a group is called a zone of cells. The cells are of equal size. So the size of a zone is proportional to the number of cells in it. Let $g_{i}$ be the number of cells in a zone, then the probability of finding particle in the zone will be proportional to $\mathrm{g}_{\mathrm{i}}$ and is called a priori probability for the zone. It is also called the degeneracy of the $\mathrm{i}^{\text {th }}$ energy group having energy $\in_{\mathrm{i}} \cdot$\\
\par Consider any one of the particles having energy $E_{i}$. This particle can occupy $g_{i}$ cells in $g_{i}$ ways. So for $n_{i}$ particles there are gi ways for each particle. Hence the total number of ways in which $n_{i}$ particles can be arranged in $g_{i}$ cells is $\overline{g_{i}} \times g_{i} \times g_{i} \times \ldots(n$ times $)=\left(g_{j}\right)^{\text {ni }}$. The total number of particles $=n$. From $\mathrm{n}$ particles, $\mathrm{n}_{\mathrm{i}}$ particles for the $\mathrm{i}^{\text {th }}$ energy state can be selected ${ }^{n} \mathrm{Cn}_{\mathrm{i}}$ ways. So the number of different ways for the first group $={ }^{n} C_{n_{1}}\left(g_{1}\right)^{n_{1}}$. Since we have arranged $n_{1}$ particles the remaining\\\\
particles is $\left(n-n_{1}\right)$. So for the second group, $n_{2}$ particles are to be taken from $\left(n-n_{1}\right)$ particles and the number of ways it can be arranged is $={ }^{n-n_{1}} C_{n_{2}}\left(g_{2}\right)^{n_{2}}=\frac{\left(n-n_{1}\right) !}{n_{2} !\left(n-n_{1}-n_{2}\right)} \times\left(g_{2}\right)^{n_{2}}$ and so on. The thermodynamic probability $\Omega$ of distribution is equal to the total number of ways of distributing $n_{1}, n_{2}, n_{3} \ldots$ particles in the energy group $\varepsilon_{1}, \varepsilon_{2}$, $\varepsilon_{3} \ldots \ldots .$
\begin{align*}
\Omega&=\frac{n !\left(g_{1}\right)^{n_{1}} \xi}{\left(n_{1}\right) ! \cdot\left(n-n_{1}\right) !} \times \frac{\left(n-n_{1}\right) !\left(g_{2}\right)^{n_{2}}}{\left(n_{2}\right) ! \cdot\left(n-n_{1}-n_{2}\right) !} \times \ldots \ldots\\
&=\frac{n ! \cdot\left(g_{1}\right)^{n_{1}} \cdot\left(g_{2}\right)^{n_{2}} \cdot\left(g_{3}\right)^{n_{3}} \ldots .\left(g_{i}\right)^{n_{i}} \ldots \ldots}{\left(n_{1}\right) ! \cdot\left(n_{2}\right) ! \cdot\left(n_{3}\right) ! \ldots .\left(n_{i}\right) ! \ldots \ldots \ldots}\\
\Omega&=n ! \cdot \Pi \frac{\left(g_{i}\right)^{n_{1}}}{\left(n_{i}\right) !}
\intertext{Taking logarithm}
\log _{\mathrm{e}} \Omega &=\log _{\mathrm{e}} \mathrm{n} !+\sum_{\mathrm{i}}\left[\log _{\mathrm{e}}\left(g_{\mathrm{i}}\right)^{\mathrm{n}_{1}}-\log _{\mathrm{e}} \mathrm{n}_{\mathrm{i}} !\right] \\
&=\log _{\mathrm{e}} n !+\sum_{\mathrm{i}}\left[\mathrm{n}_{\mathrm{i}} \log _{\mathrm{e}} g_{\mathrm{i}}-\log _{\mathrm{e}} \mathrm{n}_{\mathrm{i}} ! !\right.\\
\text { According to }&\text{Stirling's approximation, } \log _{\mathrm{e}} \mathrm{x} !=\mathrm{x} \log _{\mathrm{e}} \mathrm{x}-\mathrm{x}\\
\text{So }\log _{e} \Omega&=n \log _{e} n-n+\sum_{i}\left[n_{i} \log _{e} g_{i}-n_{i} \log _{e} n_{i}+n_{i}\right]\\
&=n \log _{\mathrm{e}} \mathrm{n}+\sum_{\mathrm{i}}\left[n_{\mathrm{i}} \log _{\mathrm{e}} \mathrm{g}_{\mathrm{i}}-\mathrm{n}_{\mathrm{i}} \log _{\mathrm{e}} \mathrm{n}_{\mathrm{i}}\right] \quad\left[\because \Sigma \mathrm{n}_{\mathrm{i}}=\mathrm{n}\right]
\intertext{Differentiating equation}
\mathrm{d}\left(\log _{\mathrm{e}} \Omega\right)&=\sum_{\mathrm{i}}\left[\mathrm{dn}_{\mathrm{i}} \log _{\mathrm{e}} g_{\mathrm{i}}-\mathrm{dn}_{\mathrm{i}} \log _{\mathrm{e}} \mathrm{n}_{\mathrm{i}}-\mathrm{n}_{\mathrm{i}} \frac{1}{\mathrm{n}_{\mathrm{i}}} \mathrm{dn} \mathrm{n}_{\mathrm{i}}\right]\\
&=\sum_{i}\left(\log _{e} g_{i}-\log _{e} n_{i}\right) d n_{i}-\sum_{i} d n_{i}\\
&=\sum_{i}\left(\log _{e} g_{i}-\log _{e} n_{i}\right) d n_{i} \text {. Since } \Sigma n_{i}\\
&=n \text { is a }\text { constant. So } \Sigma d n_{1}=0 \text {. }\\
&=\sum_{i} \log \frac{g_{i}}{n_{i}} d n_{i}=-\sum_{i} \log \frac{n_{i}}{g_{i}} d n_{i}
\intertext{The condition for the most probable distribution is}
d \Omega&=0\\
d\left(\log _{e} \Omega\right)&=0
\intertext{From equation (6) we have,}
-\sum_{i} \log \frac{n_{i}}{g_{i}} d n_{i}&=0\\
\text { or } \quad \sum_{i} \log \frac{n_{i}}{g_{i}} d n_{i}&=0
\intertext{From equation (1) and (2), we have}
\sum_{i} d n_{i}&=0
\text{(as $\mathrm{n}$ is a constant)}\\
\text{and }\sum_{i} \epsilon_{i} d n_{i}&=0
\text{(as $\mathrm{E}$ is a constant)}
\intertext{Applying the Lagrange method of undetermined multipliers, i.e., multiplying equation (9) by $\alpha$ and equation (10) by $\beta$ and adding the result to equation (8) we get,}
\sum\left(\log \frac{n_{i}}{g_{i}}+\alpha+\beta \epsilon_{i}\right) d n_{i}&=0
\intertext{But $d n_{i} \neq 0$ as they are variable quantities.}
\text { So } \log _{\mathrm{e}} \frac{\mathrm{n}_{\mathrm{i}}}{\mathrm{g}_{\mathrm{i}}}+\alpha+\beta \epsilon_{\mathrm{i}}&=0\\
\log _{\mathrm{e}} \frac{\mathrm{n}_{\mathrm{i}}}{\mathrm{g}_{\mathrm{i}}} &=-\left(\alpha+\beta \epsilon_{\mathrm{i}}\right) \\
\frac{\mathrm{n}_{\mathrm{i}}}{\mathrm{g}_{\mathrm{i}}} &=\mathrm{e}^{-\left(\alpha+\beta \epsilon_{\mathrm{i}}\right)}\\
\mathbf{n}_{\mathbf{i}}&=\frac{\mathbf{g}_{\mathbf{i}}}{e^{\left(\alpha+\beta \epsilon_{i}\right)}}
\intertext{This is the equation for the most probable distribution according to Maxwell-Boltzmann statistics.}
\text { Let } e^{-\alpha}&=A \text {, another constant }\\
\text { Then, } n_{i}&=\frac{A g_{i}}{e^{\beta \epsilon_{i}}}=A g_{i} e^{-\beta \epsilon_{i}}
\end{align*}
\begin{note}
	Suppose the energy of the system has continuous values, in the range from $\in$ to $\in+\mathrm{d} \in$, the density of states $\mathrm{g}_{\mathrm{i}}$ can be replaced by $g(\in) d \in$ and $\epsilon_{i}$ by $\in$. The distribution law given by equation (13) can be written as, $n(\in) d \in=\frac{\operatorname{Ag}(\in) d \in}{e^{\beta \epsilon}}$
\end{note}
\section{Maxwell's formula for velocity distribution}
Maxwell-Boltzmann distribution law is
$$
n_{i}=A g_{i} \mathrm{e}^{-\beta \epsilon_{i}}
$$
When the system has continuous energy values, in the range from $E$ to $E+d E$, then $g_{i}$ is replaced by $g(\in) d \in$ and $\epsilon_{i}$ by $\varepsilon$.

Then the number of molecules $\mathrm{dN}$ with energy between $\in$ and $\varepsilon+d \varepsilon$ can be written as,
\begin{align*}
\mathrm{dN}&=A \mathrm{e}^{-\beta \varepsilon} g(\varepsilon) \mathrm{d} \varepsilon\\
\text{But }A&=\frac{N}{Z}, \beta=\frac{1}{k T}\\
\mathrm{dN}&=\frac{\mathrm{N}}{\mathrm{Z}} \mathrm{e}^{-\epsilon \mathrm{kT}} \mathrm{g}(\in) \mathrm{d} \in\\
\text { But from equation (30) } \mathrm{g}(\epsilon) \mathrm{d} \in&=\frac{2 \pi \mathrm{V}}{\mathrm{h}^{3}}(2 \mathrm{~m})^{3 / 2} \epsilon^{1 / 2} \mathrm{~d} \in
\intertext{Substituting in equation(48)}
\mathrm{dN}&=\frac{\mathrm{N}}{\mathrm{Z}} \mathrm{e}^{-\epsilon \mathrm{kT}} \times \frac{2 \pi \mathrm{V}}{\mathrm{h}^{3}}(2 \mathrm{~m})^{3 / 2} \epsilon^{1 / 2} \mathrm{~d} \in
\intertext{Substituting the value of z}
Z&=\frac{V}{h^{3}}(2 \pi k m T)^{3 / 2} \text { from equation }(33)\\
\mathrm{dN}&=\frac{\mathrm{N} \times \mathrm{h}^{3}}{\mathrm{~V}(2 \pi \mathrm{kmT})^{3 / 2}} \times \mathrm{e}^{-\epsilon \mathrm{kT}} \times \frac{2 \pi \mathrm{V}}{\mathrm{h}^{3}}(2 \mathrm{~m})^{3 / 2} \epsilon^{1 / 2} \mathrm{~d} \epsilon\\
\frac{d N}{d \in}&=\frac{2 \pi N}{(\pi k T)^{3 / 2}} \epsilon^{1 / 2} e^{-\epsilon / k T}
\intertext{This is the formula for the energy distribution of the molecules in an ideal gas.}
\intertext{The variation $(\mathrm{dN} / \mathrm{d} \varepsilon)$ with $\varepsilon$ is as shown in Fig. $4.12$ for two different temperature.}
\end{align*}
\section{Velocity distribution formula}
\begin{align*}
\text { The energy of a molecule } \epsilon&=\frac{1}{2} m v^{2}, \frac{d \in}{d v}=m v
\intertext{Let $\mathrm{dN}$ be the number of molecules with speeds between $v$ and $v+d v$.}
\frac{\mathrm{dN}}{\mathrm{dv}}&=\frac{\mathrm{dN}}{\mathrm{d} \varepsilon} \cdot \frac{\mathrm{d} \varepsilon}{\mathrm{dv}}\\
\intertext{\text { Substituting equation (49) and }(50) \text { in (51). }}
\frac{d N}{d v}&=m v \times \frac{2 \pi N}{(\pi k T)^{3 / 2}}\left(\frac{1}{2} m v^{2}\right)^{1 / 2} e^{-m v^{2} / 2 k T}\\
\frac{\mathrm{dN}}{\mathrm{dv}}&=\frac{2 \mathrm{~N}}{\sqrt{2 \pi}}\left(\frac{\mathrm{m}}{\mathrm{kT}}\right)^{3 / 2} \mathrm{v}^{2} \mathrm{e}^{(-1 / 2) \mathrm{mv}^{2} / \mathrm{kT}}
\end{align*}
\par This is the Maxwells formula for the velocity distribution of the molecules in an ideal gas. This equation gives the number of molecules having a velocity between $v$ and $v+d v$ which does not depend on the direction of motion. The variation of $\mathrm{dN} / \mathrm{dv}$ with velocity $v$ is as shown in Fig. 4.13.\\
\par The variation of $\mathrm{dN} / \mathrm{dv}$ with speed for two different temperatures $\left(\mathrm{T}_{2}>\mathrm{T}_{1}\right)$ as shown in Fig. 4.14. The higher the temperature the wider is the spread of the values of the speed.\\
From the graph the following conclusions are drawn.
\begin{enumerate}
	\item There is no molecule having zero speed.
	\item With increase in speed, the number of molecules in a given speed interval $\Delta \mathrm{v}$ increases upto a certain maximum value. The speed corresponding to the maximum number of molecules is called the most probable speed $v_{p}$ at that temperature.
	\item  When the speed increases beyond $v_{p}$, the number of molecules decreases exponentially towards zero. This means a molecule can have infinite speed according to classical statistics.
	\item With increase in temperature the value of $v_{p}$ increases and the range of speed also becomes greater. The graph becomes broader.
	\item The area under the graph is equal to the total number of molecules in the gas.
\end{enumerate}
\subsection{Most Probable Speed $\left(\mathbf{v}_{\mathrm{p}}\right)$}

The most probable speed $\left(v_{p}\right)$ of the molecules is that speed at which the number of molecules per unit range of speed is maximum.
$$
v_{p}=\sqrt{\frac{2 k T}{m}}
$$
\par where $T$ is the temperature of the gas, $k$ is the Boltzmann's constant and $m$ is the mass of the molecule.\\
\textbf{Average speed}
\par The arithmetic mean of the speeds of the molecules in the gas at a given temperature is called average speed $\overline{\mathbf{v}}$.